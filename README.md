# AniSOAP Optimization

> **Goal:** Make anisotropic SOAP (AniSOAP) descriptors *fast, scalable, and easy to reproduce* across CPU/GPU backends while preserving descriptor fidelity.

<!--
HOW TO USE THIS DRAFT
1) Paste Arthurâ€“Tejas message snippets into the chat; I (ChatGPT) will fold them into the marked TODO blocks.
2) Replace any ðŸš§ TODO blocks with concrete content.
3) Ensure filenames/paths match your repo (caseâ€‘sensitive).
-->

## Why this repo exists (Problem â†’ Impact)

* **Problem:** Descriptor generation for atomistic ML pipelines can be a bottleneck (wallâ€‘time, memory, vectorization limits, I/O), especially at scale and across diverse species.
* **Impact:** Faster AniSOAP unlocks bigger datasets, larger hyperparameter sweeps, and practical deployment in downstream interatomic potentials and property models.
* **This repo solves:** A principled, reproducible optimization pathâ€”with baselines, profiling, and validated speedups on real datasets.

## TL;DR (What we did)

* Built a **reproducible benchmarking harness** (datasets, seeds, metrics).
* Implemented **profiling** (cProfile/pyâ€‘spy/line_profiler) and **microâ€‘benchmarks**.
* Compared **systems Ã— backends** and **wallâ€‘time vs. #species**.
* Produced **publicationâ€‘quality figures** and CSV tables.
* Documented **tuning levers** (algorithmic, memory, parallelism, vectorization, batching).

> Key artifacts live in `results/figures/`, `results/tables/`, and `results/logs/`.

## Context & scope

* **AniSOAP**: anisotropic Smooth Overlap of Atomic Positions descriptor.
* **Scope of this repo:** performance engineering + correctness checks for descriptor generation; does *not* reimplement the learning models themselves.
* **Outâ€‘ofâ€‘scope:** exhaustive chemistry benchmarks, downstream ML leaderboard.

## Repo structure (highâ€‘level)

```
.
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ make_plots.py                 # generates figures from metrics JSON/CSVs
â”‚   â”œâ”€â”€ ðŸš§ (add) run_benchmarks.py     # entrypoint to run all benchmark suites
â”‚   â””â”€â”€ ðŸš§ (add) profile_*.py          # minimal repros for targeted profiling
â”œâ”€â”€ anisoap_opt/                      # (optional) library code if any
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/
â”‚   â”‚   â”œâ”€â”€ wall_time_by_system.png
â”‚   â”‚   â””â”€â”€ wall_time_vs_species.png
â”‚   â”œâ”€â”€ tables/
â”‚   â”‚   â””â”€â”€ combined_from_metrics.csv
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ env/
â”‚   â””â”€â”€ ðŸš§ (add) environment.yml | pyproject.toml
â”œâ”€â”€ data/                             # symlinks or small example snippets only
â”œâ”€â”€ README.md                         # â† this file
â””â”€â”€ LICENSE
```

## Installation

**Option A (conda):**

```bash
conda env create -f env/environment.yml
conda activate anisoap-opt
```

**Option B (uv/pip):**

```bash
uv venv && source .venv/bin/activate
uv pip install -e .
```

> ðŸš§ TODO: list core dependencies (Python â‰¥3.x, PyTorch/CUDA versions, compilers, OpenMP/MKL, etc.).

## Data

* We use internal or public molecules/systems to measure descriptor throughput.
* **Large datasets are not checked in.** Provide paths via env vars or CLI.

**Example layout**

```
DATA_ROOT/
  â”œâ”€â”€ system_A/
  â”œâ”€â”€ system_B/
  â””â”€â”€ ...
```

> ðŸš§ TODO: Document any dataset sources, licenses, and download helpers.

## Reproducing the results

### 1) Run benchmarks

```bash
python scripts/run_benchmarks.py \
  --data $DATA_ROOT \
  --config configs/batch_cpu.yaml \
  --out results/metrics/cpu.json

python scripts/run_benchmarks.py \
  --data $DATA_ROOT \
  --config configs/batch_gpu.yaml \
  --out results/metrics/gpu.json
```

### 2) Aggregate into a single table

```bash
python scripts/aggregate_metrics.py \
  --inputs results/metrics/*.json \
  --out results/tables/combined_from_metrics.csv
```

### 3) Make plots

```bash
python scripts/make_plots.py \
  --table results/tables/combined_from_metrics.csv \
  --figdir results/figures
```

## Results (figures & tables)

### QUICK CHECKLIST â€” add your local figures so they render on GitHub

1. **Locate** your artifacts on macOS (replace the path root if needed):

   ```bash
   # from anywhere
   mdfind 'kMDItemFSName ==[c] "*anisoap*" || kMDItemFSName ==[c] "*profiling*"' | head -n 50
   # or within your project dir
   cd /path/to/cersonskylab-anisoap-optimization
   find . -iname "*.png" -o -iname "*.svg" -o -iname "*.prof" -o -iname "*timings*.csv" -o -iname "*.json"
   ```
2. **Copy** the figures you want into the repo (caseâ€‘sensitive paths):

   ```bash
   mkdir -p results/figures results/logs results/tables
   # examples â€” adjust src paths to your Mac
   cp ~/Desktop/profiling/wall_time_by_system.png    results/figures/
   cp ~/Desktop/profiling/wall_time_vs_species.png   results/figures/
   cp ~/Desktop/profiling/prof_benzenes_callgraph.png results/figures/
   cp ~/Desktop/profiling/prof_ellipsoids_callgraph.png results/figures/
   cp ~/Desktop/profiling/bench.svg                  results/logs/
   cp ~/Desktop/profiling/timings_chtc.csv           results/tables/
   cp ~/Desktop/profiling/timings_local.csv          results/tables/
   cp ~/Desktop/profiling/summary_local.csv          results/tables/
   cp ~/Desktop/profiling/prof_benzenes_200.prof     results/logs/
   cp ~/Desktop/profiling/prof_ellipsoids_200.prof   results/logs/
   ```
3. **Commit** so GitHub can render them:

   ```bash
   git add results/figures results/logs results/tables README.md
   git commit -m "Add profiling figures, logs, and tables; update README"
   git push origin main
   ```

> Image markdown must be **outside** code fences and filenames **caseâ€‘sensitive**.

```text
# Do NOT put image lines inside this code block.
```

![Wall time by system & backend](results/figures/wall_time_by_system.png)

![Wall time vs #Species](results/figures/wall_time_vs_species.png)

![cProfile call graph â€” Benzenes](results/figures/prof_benzenes_callgraph.png)

![cProfile call graph â€” Ellipsoids](results/figures/prof_ellipsoids_callgraph.png)

*Takeaway:* Torch CPU typically outperforms NumPy for `einsum`â€‘heavy sections; MPS/CUDA plots should be added once the full Torch path is ported.

*Takeaway:* After normalizing by **NÂ²**, species curves do not show superâ€‘quadratic behavior on the standardized files.

**Combined metrics table:** `results/tables/combined_from_metrics.csv`

> Image markdown must be **outside** code fences and filenames **caseâ€‘sensitive**.

```text
# Do NOT put image lines inside this code block.
```

![Wall time by system & backend](results/figures/wall_time_by_system.png)

![Wall time vs #Species](results/figures/wall_time_vs_species.png)

**Combined metrics table:** `results/tables/combined_from_metrics.csv`

> ðŸš§ TODO: add a short narrative under each figure explaining: dataset, hardware, sample size, takeaways.

## Species scaling & normalization

We report `time / NÂ²` (where `NÂ²` is the size of the pairwise atom grid touched by distance/einsum steps) to remove raw atomâ€‘count effects. Using `create_fake_benzenes.py`, we generate multiâ€‘species files with **constant N** so that label changes (1â†’4 species) are the only variable. Under this control, curves remain stable and do **not** exhibit worseâ€‘thanâ€‘quadratic growth with species.

## What we optimized (and why)

* **Vectorization & batching**: reduce Python overhead; maximize BLAS/GPU utilization.
* **Parallelism**: OpenMP/threading on CPU; streams/blocks on GPU.
* **Memory traffic**: layout/contiguity, pinning, avoiding needless copies.
* **Algorithmic choices**: cutoff radii, basis sizes, truncations that preserve accuracy but lower cost.
* **I/O & caching**: chunked reads/writes; memoize reusable intermediates.

> ðŸš§ TODO: Tie each bullet to concrete code changes, PRs, or commits.

## Profiling & methodology

* **cProfile/pyâ€‘spy**: hotspot discovery at function level.
* **line_profiler**: lineâ€‘level attribution for kernels.
* **nvprof/ncu** (if GPU): kernel occupancy and memory throughput.
* **A/B experiments**: one change at a time with fixed seeds.

**Reproduce profiling**

```bash
py-spy record -o results/logs/bench.svg -- python scripts/run_benchmarks.py ...
python -m cProfile -o results/logs/bench.prof scripts/run_benchmarks.py ...
```

> ðŸš§ TODO: Link representative `.svg` flamegraphs and `.prof` summaries in `results/logs/`.

## Validation (did we keep correctness?)

* Crossâ€‘check descriptor arrays against **reference** implementation within tolerance.
* Unit tests for **shape/dtype** & invariances.
* Spotâ€‘check downstream task metrics unchanged (or documented tradeoffs).

**Tests:** Compare Torch paths to NumPy reference using `np.testing.assert_allclose` with tight tolerances (suggested: `rtol=1e-6`, `atol=1e-8` for fp64; relax appropriately for fp32). Include shape/dtype asserts and invariance checks (rot/perm where applicable). Run via `pytest -q`.

## Artifacts & file map (from the email thread)

| Kind                        | Suggested path in repo                          | Notes                                  |
| --------------------------- | ----------------------------------------------- | -------------------------------------- |
| Wallâ€‘time by system figure  | `results/figures/wall_time_by_system.png`       | Generated by `scripts/make_plots.py`   |
| Wallâ€‘time vs #species       | `results/figures/wall_time_vs_species.png`      | NÂ²â€‘normalized species curves           |
| Benzenes call graph (PNG)   | `results/figures/prof_benzenes_callgraph.png`   | From `prof_benzenes_200.prof`          |
| Ellipsoids call graph (PNG) | `results/figures/prof_ellipsoids_callgraph.png` | From `prof_ellipsoids_200.prof`        |
| Flamegraph (pyâ€‘spy)         | `results/logs/bench.svg`                        | Optional but useful                    |
| cProfile (benzenes)         | `results/logs/prof_benzenes_200.prof`           | Text + graphable                       |
| cProfile (ellipsoids)       | `results/logs/prof_ellipsoids_200.prof`         | Text + graphable                       |
| Timings (CHTC)              | `results/tables/timings_chtc.csv`               | Perâ€‘run raw rows                       |
| Timings (local)             | `results/tables/timings_local.csv`              | Perâ€‘run raw rows                       |
| Summary (local)             | `results/tables/summary_local.csv`              | Mean Â± std                             |
| Aggregated metrics          | `results/tables/combined_from_metrics.csv`      | From `aggregate_metrics.py`            |
| Perâ€‘job metrics             | `results/logs/*.metrics.json`                   | Emitted by runner                      |
| Raw wall stamps             | `results/logs/*.wall`                           | Emitted by runner                      |
| Repro bundle (CHTC)         | `profiling_artifacts.tgz`                       | Contains results/, logs/, submit files |
| Repro bundle (local)        | `profiling_local.zip`                           | Contains local CSVs + harness          |

If your locally saved filenames differ, either **rename to these** or update the README image lines accordingly.

## Hardware & environment

* **CPU:** ðŸš§ TODO (model, cores, threads, RAM)
* **GPU:** ðŸš§ TODO (model, driver, CUDA)
* **OS:** ðŸš§ TODO (Linux distro & version)
* **Libraries:** ðŸš§ TODO (MKL/OpenBLAS, PyTorch)

> Results are hardwareâ€‘sensitive; please include your specs when reporting issues.

## Command cookbook

Common invocations we found useful:

```bash
# 1) Small sanity benchmark
python scripts/run_benchmarks.py --preset tiny --out results/metrics/tiny.json

# 2) Multiâ€‘species sweep
python scripts/run_benchmarks.py --sweep species --out results/metrics/species.json

# 3) Threads sweep (CPU)
OPENMP_NUM_THREADS=1,2,4,8,16 ... python scripts/run_benchmarks.py --preset cpu

# 4) Batch size sweep (GPU)
python scripts/run_benchmarks.py --preset gpu --sweep batch
```

## Repo status & roadmap

* âœ… Baseline metrics + plots checked in.
* âœ… Repro scripts for figures.
* ðŸš§ Public benchmark configs (CPU/GPU presets).
* ðŸš§ Automated CI to run microâ€‘benchmarks on commits.
* ðŸš§ Documentation site (mkdocs) with deeper guides.

## How to cite

> ðŸš§ TODO: add citation(s) for AniSOAP and this optimization report once available.

```bibtex
@inproceedings{cersonsky202Xanisoap,
  title     = {Anisotropic SOAP and Optimization Benchmarks},
  author    = {Cersonsky, Rose and Lin, Arthur and Dahiya, Tejas and ...},
  year      = {202X},
  booktitle = {...}
}
```

## Acknowledgements

Thanks to **Cersonsky Lab** (UWâ€“Madison), **Arthur Lin**, and collaborators for guidance and reviews.

---

### Appendix: Repro tips

* Fix seeds and versions; export `PYTHONHASHSEED=0`.
* Keep environments immutable during a run.
* Pin threads with `taskset` or `numactl` when comparing CPU backends.

### Appendix: Troubleshooting

* **Figures not rendering in GitHub preview?** Ensure image lines are outside code fences and filenames match exactly.
* **Missing datasets?** Provide `--data` or set `DATA_ROOT`.
* **Slow CSV writes?** Use `mode='w', index=False` and consider gzip (`.csv.gz`).
